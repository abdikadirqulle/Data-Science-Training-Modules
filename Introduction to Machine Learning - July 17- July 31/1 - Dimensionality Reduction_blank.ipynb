{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Unsupervised Learning\n",
    "\n",
    "Unsupervised learning is a branch of machine learning where the goal is to find patterns or structure in data without explicit supervision or labeled examples. Unlike supervised learning, unsupervised learning does not rely on predefined output labels during the training process. Instead, it focuses on **discovering relationships**, groupings, or representations within the data itself.\n",
    "\n",
    "It is particularly useful when dealing with unannotated or unlabeled datasets, making it a powerful tool for data exploration, data preprocessing, and feature engineering.\n",
    "\n",
    "Common Applications of Unsupervised Learning:\n",
    "\n",
    "- Clustering: Clustering is one of the main applications of unsupervised learning, where the algorithm groups similar data points together into clusters based on their inherent similarities. It is widely used in customer segmentation, image segmentation, anomaly detection, and recommendation systems.\n",
    "\n",
    "- Dimensionality Reduction: Unsupervised learning techniques, such as Principal Component Analysis (PCA) and t-distributed Stochastic Neighbor Embedding (t-SNE), are used to reduce the dimensionality of high-dimensional data. This helps in visualizing data, speeding up computations, and improving the performance of machine learning models.\n",
    "\n",
    "- Anomaly Detection: Unsupervised learning can be used to detect anomalies or outliers in data, which are observations that deviate significantly from the majority of the data. This is valuable in fraud detection, fault detection in industrial systems, and network intrusion detection.\n",
    "\n",
    "- Density Estimation: Density estimation algorithms estimate the probability density function of the data, helping to model the data distribution. Kernel Density Estimation (KDE) and Gaussian Mixture Models (GMM) are commonly used for this purpose.\n",
    "\n",
    "- Natural Language Processing (NLP): In NLP, unsupervised learning is used for tasks like topic modeling, word embeddings (e.g., Word2Vec), and document clustering, enabling efficient text analysis and information retrieval.\n",
    "\n",
    "Unsupervised learning techniques play a crucial role in data analysis, exploratory data analysis (EDA), and pattern recognition, providing valuable insights and knowledge discovery from unlabeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " **Principle Component Analysis** (PCA) is an unsupervised transformation, used to decompose a multivariate dataset in a set of successive orthogonal components that explain a maximum amount of the variance. It can be explained in a simple way using the following analogy:\n",
    "\n",
    "Imagine you have a collection of photographs of different objects, animals, and landscapes. Each photo has a lot of colors and details. However, you notice that many of the photos have similarities and share common patterns. PCA is like taking these photos and transforming them into a smaller set of pictures that capture the most important and distinctive features of the entire collection.\n",
    "\n",
    "In more technical terms, PCA is a mathematical technique that reduces the complexity of data by finding the most meaningful directions, called \"principal components,\" in which the data varies the most. These principal components represent the primary patterns and relationships in the data.\n",
    "\n",
    "That is, we find new features to represent the data that are a **linear combination** of the old data (i.e. we rotate it).\n",
    "\n",
    "The way PCA finds these new directions is by looking for the directions of maximum variance.\n",
    "Usually only few components that explain most of the variance in the data are kept. The first principal component represents the most significant pattern or variation in the data, while the second principal component captures the second most significant pattern, and so on. By choosing a few principal components, we can represent the data in a lower-dimensional space, which is easier to understand and visualize.\n",
    "\n",
    "In scikit-learn, PCA is implemented as a transformer object that learns `n_components` in its `fit` method, \n",
    "and can be used on new data to project it on these components.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we import PCA from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Then we fit the PCA model with our data. As PCA is an unsupervised algorithm, there is no output ``y``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can now transform the data, projected on the principal components, and visualise the new representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might wonder how much of the information from the original photos is preserved in each of these \"summarised\" features.\n",
    "\n",
    "The variance ratio in PCA represents how much information is retained in each principal component. It tells us the proportion of the total variation in the data that is explained by each principal component.\n",
    "\n",
    "In more technical terms, the variance ratio for a principal component is the fraction of the total variance in the data that is captured by that component. The total variance is the sum of the variances of all the original features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fist principal component retains 92% of the total variation in the data. The second is less effective to describe the variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: PCA for facial recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One real-life application of PCA is in the field of facial recognition. Facial recognition technology is used in various applications, such as security systems, authentication mechanisms, and image tagging in social media platforms. PCA can be employed to reduce the dimensionality of facial images while preserving the most important features, making the recognition process more efficient.\n",
    "\n",
    "Let's create an example of using PCA for facial recognition using the famous \"Labeled Faces in the Wild\" (LFW) dataset available in Scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Labeled Faces in the Wild (LFW) dataset\n",
    "# write code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract the images and their corresponding labels\n",
    "# write code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "# write code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the first 10 principal components as eigenfaces\n",
    "# write code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct an example image using the reduced components\n",
    "# write code here\n",
    "\n",
    "# Visualize the original and reconstructed images\n",
    "# write code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we used the Labeled Faces in the Wild (LFW) dataset, which contains facial images of different individuals. \n",
    "\n",
    "We applied PCA to the images to reduce their dimensionality to 150 principal components, called \"eigenfaces.\" These eigenfaces represent the **most significant features** of facial images in the dataset.\n",
    "\n",
    "We visualized the first 10 eigenfaces, which are characteristic patterns that contribute most to the variation in the dataset. These eigenfaces can be thought of as a set of \"building blocks\" that can be combined to reconstruct any facial image from the dataset.\n",
    "\n",
    "Finally, we reconstructed an example image using the reduced principal components. The reconstruction process shows that the main facial features are preserved despite the significant dimensionality reduction.\n",
    "\n",
    "Facial recognition systems can use these reduced representations to efficiently match and identify faces, making PCA a powerful tool in real-life applications like biometric authentication and security systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
